>**Parsing** - language definition
>To resolve (*as a sentence*) into component parts of speech and describe them grammatically.

In a computer science context, we take this to mean answering whether or not :$$w \in L(G)$$
For a **CFG** $G$ by analysing the structure of $w$ according to $G$ i.e., to *recognise* the language generated by a grammar $G$.

A ***parser*** is a program that carries out parsing. For a **CFG** this amounts to a realisation of a **PDA** $P$ which processes an input string and determines whether a given word $w \in L(P)$.
For most practical applications, parser will also return a structured representation of a word $w \in L(G)$. This could be the derivation tree for the word, or, more commonly, a simplified version of this called **Abstract Syntax Tree.*** Further, for a word not in the language, $w \in L(G)$a practical parser would normally provide an error message explaining why. 
Application for parsers: front-end of compilers and interpreters for any type of programming/scripting languages, as they tokenised representation of a program into structured representation for further analysis and translation or execution. 

Here, discuss how to construct parser from given CFG using recursive-descent parsing method. 

## Parsing strategies
There are two basic strategies for parsing: *top-down* and *bottom-up*
- A **top-down** parser attempts to carry out a derivation matching the input from the start symbol; it constructs the parse tree for the input from the root downwards in preorder (R->L->R).
- A **bottom-up** parser attempts to construct the parse tree from the leaves by using the productions 'backwards'

Essentially been doing top-down parsing thus far whenever derived some specific word in a language from start symbol of a grammar generating that language. 
E.g., 
![[Pasted image 20250501004030.png]]
![[Pasted image 20250501004037.png]]
![[Pasted image 20250501004101.png]]
In contrast, bottom-up parser would start from the leaves, and step-by step group them together by applying productions in reverse???
![[Pasted image 20250501004139.png]]

We cover recursive descent parsing here, type of top-down parsing.

## Basics of recursive-descent parsing
***Recursive-descent parsing*** is a way to implement top-down parsing. We are just focusing on language recognition problem: $w \in L(G)?$

```haskell
parser :: [Token] -> Bool
```
Consider a typical production in some **CFG** $G$ 
$$

S \to AB

$$
$S$ can be expanded into $AB$ 
For any nonterminal $X$, Let $L(X)$ be the language $\{w \in T^* \ | \ X \xRightarrow{*}_G w \}$ 
(starting from $X$, can derive $w$) Note that:

$$

\begin{array}{rll}

w \in L(S) \Leftarrow \exists w_{1},w_{2}\ \cdot & w=w_{1}w_{2} \\

 & \wedge w_{1} \in L(A) \\

 & \wedge w_{2} \in L(B)

\end{array}

$$
That is given a parser for $L(A)$ and a parser for $L(B)$, we can constructor a parser for $L(S)$ by asking first parser if a prefix $w_1$ of $w$ belongs to $L(A)$ and then asking the other parser if the remaining suffix $w_2$ belongs to $L(B)$. If the answer to both questions is yes, then $w$belongs to $L(S)$.
But need to find right way to divide the input word $w$! in general, there are $|w|+1$ possibilities, could blindly try them all, but prefix and suffix have to be split in all possible ways, would not be computationally viable for anything but short words and sequences of tokens.
Instead we need to let the input guide the search, initially adopt the following idea:
- Each parser tries to derive a *prefix* of the input according to the productions for the nonterminal
	![[Pasted image 20250501010916.png]]
- Each parser returns the remaining suffix if successful, allowing this to be passed to the next parser for analysis.
	![[Pasted image 20250501010953.png]]
Once one parser successfully consumes a part of the input and produces a suffix (the remaining tokens), the next parser can take that suffix and try to match its part.

**NOTE:** One parser function per nonterminal in the grammar, not per production. 

There could be **more** than one prefix derivable from a non-terminal, and if so, how can we know which one to pick, as picking the wrong one might make it impossible to derive the suffix from the non-terminal that follows. Return to these points later. 

Can now construct a parser for $L(S)$ in terms of $L(A)$ and $L(B)$.

```haskell
parseS :: [Token] -> Maybe [Token]
parseS ts = 
	case parseA ts of 
		Nothing -> Nothing 
		Just ts' -> parseB ts'
```

Called recursive-descent parsing because the parse functions usually end up being mutually recursive (see chatgpt if confused). What does this have to do with realising a **PDA**? Fundamental to the implementation of a recursive computation is that it keeps track of the *state* of the computation and allows for *subcomputations*. The stack is not explicitly visible, but is a central datastructure, thus a recrusive-descent parser is a kind of **PDA.**

Let us develop the above example further:
First for simplicity, let us pick the type `Char` for token:
```haskell
type Token = Char
-- Thus [Token] = [Char]
```
 Now suppose the **productions** for $A$ and $B$ are:

$$

\begin{array}{rll}

A  & \to & a \\

B & \to & b

\end{array}

$$

Here is `parseA`:

```haskell

parseA :: [Token] -> Maybe [Token]

parseA ('a' : ts) = Just ts

praseA _          = Nothing

```

And here is `parseB`:

```haskell

parseB :: [Token] -> Maybe [Token]

parseB ('b' : ts) = Just ts

parseB _          = Nothing

```
Parsing $abcd$:
	Prefix of $abcd$ can be derived from $A$ leaving remaining suffix $bcd$, that no prefix of $abcd$ can be derived from B, but that a prefix of $abcd$ can also be derived from $S$ leaving a suffix $cd$.
>parseA "abcd"
Just "bcd"

>parseB "abcd"
Nothing

> parseS "abcd"
Just "cd"
## Handling choice
Of course, usually more than one production for a nonterminal, as in:
$$

S\to AB\mid CD

$$
First, consider the case when the choice is obvious:
$$

S\to aAB\mid cCD

$$
That is, assume it is manifest from the grammar that we can choose between productions with a one-symbol *lookahead* for guidance on what to choose. Let us construct a parser for the following grammar as an example.
$$\begin{array}{rll}

S & \to & aA & \mid & bBA \\

A & \to & aA & \mid & \epsilon \\

B & \to & bB & \mid & \epsilon

\end{array}$$
We need one parsing function for each **non-terminal**:
```haskell

parseS :: [Token] -> Maybe [Token]

parseA :: [Token] -> Maybe [Token]

parseB :: [Token] -> Maybe [Token]

```
**Pattern matching** makes use of one-symbol lookahead to chose between the **two productions** for **$S$**:

```haskell
parseS :: [Token] -> Maybe [Token]
parseS ('a' : ts) =
    parseA ts
parseS ('b' : ts) =
    case parseB ts of
        Nothing  -> Nothing
        Just ts' -> parseA ts'
parseS _ = Nothing
```
So here we can see we do $S\to aA\mid bBA$, if we start with $a$ then we just do `parseA` and if we start with $b$ then we do `parseB` and then `parseA`

Now for `parseB` and `parseA` now we have $A\to aA\mid\epsilon$ meaning it is **not** as **syntax** **error** if the next token is not $a$, since we can choose $\epsilon$.
For example $a\epsilon$ is fine. 

So both functions can **succeed** without consuming any input:
```haskell
parseA :: [Token] -> Maybe [Token]
parseA ('a' : ts) = parseA ts 
parseA ts = Just ts --empty production

parseB :: [Token] -> Maybe [Token] 
parseB ('b' : ts) = parseB ts 
parseB ts = Just ts --empty production
```
Remember, recognises prefix, and returns suffix it cannot currently parse, often to another parser.

Now consider more challenging scenario:$$S\to aA\mid aBA$$
In the parsing function **parseS** for nonterminal $S$, should **parseA** or **parseB** be called once $a$ has been read?
Could try the alternatives in order:
```haskell

parseS ('a' : ts) =
    case parseA ts of
        Just ts' -> Just ts'
        Nothing  ->
            case parseB ts of
                Nothing  -> Nothing
                Just ts' -> parseA ts'
```
If parse A succeeds, simply parse, if it doesn't, then try parseB, if that works, then try A again with returned suffix.
The choice to try parseA first is arbitrary here.
This is a limited form of ***backtracking***.

Now although the above works for this example there for other **CFG**s this will **not** work.

This is because our *limited* **backtracking** is not an **exhaustive search**.

For many **grammars**, there simply is no one order that will *always* work and will recognise all $w \in L(G)$, meaning that a parser that nevertheless commits to one particular order is liable to get stuck in **blind alleys**.

Considering the following **CFG**:
$$

\begin{array}{rll}

S & \to & AB \\

A & \to & aA & \mid & \epsilon \\

B & \to & ab

\end{array}

$$
and the corresponding parser functions for each non-terminal:
```haskell
parseA ('a' : ts) = parseA ts
parseA ts = Just ts 

parseB ('a' : 'b' : ts) = Just ts 
parseB ts = Nothing

parseS ts = 
	case parseA ts of 
		Nothing -> Nothing 
		Just ts' -> parseB ts'
```
Will this work for this order?

Let us try this on $w = ab$, clearly derivable from the grammar:$$

S \to AB\to B\to ab

$$
This will fail. Running the parser on $ab$, we start with `parseA`, consuming the `a`, and then call `parseB` with the suffix `b`, which fails, as `parseB` pattern matches looking for an `a`, which has already been **consumed**. The empty production should've been chosen at $A$. `parseA` committed to $A \to a$ too early and never tried $A \to \epsilon$, leading to the parser getting stuck in a **blind alley**.

Would it have been better to do $A \to \epsilon$ first? Then the parser would successfully determine for the word $ab$ but fail on other words that should be accepted such as $aab$. 

To successfully parse that word, **parseA** must somehow consume the first a but not the second, and neither ordering of the productions for A will achieve that.

One principled approach addressing this dilemma is to try **all** alternatives, aka **full backtracking**. 

Each parsing function here returns a list of all possible suffixes, concatenating them to a list of strings, and checking if the input strings is in that list of strings, then it is able to be parsed.
```haskell
parseX :: [Token] -> [[Token]]
```

Translate $A \to a \ \ | \ \beta$  into
```haskell
parseA ts = parseAlpha ts ++ parseBeta ts
```
However this full backtracking approach is computationally expensive, and in error reporting ,becoes difficult to pinpoint exact location of syntax error: where exactly lies the problem if it only after an exhaustive search becomes apparent there is no way to parse that word?

### Recursive-descent parsing and left-recursion
Consider the grammar $$ A \to Aa \ \ | \ \ \epsilon$$
and the corresponding recursive-descent parsing function:
```haskell
parseA :: [Token] -> Maybe [Token] 
parseA ts = 
	case parseA ts of 
		Just ('a' : ts') -> Just ts' 
		_ -> Just ts
```
This will call itself without consuming any input, it will just loop forever, as the grammar is left-recursive. 
	 A production for a nonterminal $A$ where the same nonterminal is the first symbol of the right-hand side, in the leftmost position, is called immediately ***left-recursive***. $A → Aa$ 
Recursive descent parsers thus cannot deal with ***left-recursive*** grammars. The standard way of resolving this is to **transform** a ***left-recursive grammar** into an equivalent grammar that is not left recursive and then deriving the parser from the non-left recursive version of the grammar.*

### Predictive parsing
In a recursive-descent parsing setting, we want a parsing function to be successful **exactly** when a prefix of the input can be derived from the corresponding nonterminal. This is achieved by:
- Adopting a suitable parsing strategy, specifically regarding how to handle choice between two or more productions for one nonterminal. E.g., full backtracking
- Impose ***restrictions*** on the grammar to ensure success of chosen parsing strategy.
***Predictive parsing*** is when all parsing decisions can be made based on a lookahead of limited length, typically one symbol. We have seen cases where predictive parsing, clearly possible. Manifestly the case when RHS of each possible production starts with a distinct terminal, so know which to apply.
$$ S \to \ aB \ | \ cD$$ Choice not always obvious, and if make arbitrary choices regarding which order to try productions, parser likely to be flawed.

Now look into exactly when the next input symbol is sufficient to make all choices. And as a result, if faced with a grammar where a one symbol lookahead is not enough, we will know this, and can take corrective action e.g., transform the grammar.
$$

\begin{array}{rll}

S & \to & AB & \mid & CD \\

A & \to & a & \mid & b \\

C & \to & c & \mid  & d

\end{array}

$$
If starts with an $a$ or a $b$ should clearly attempt to parse by the production $S \to AB$ , and if it starts with a $c$ or $d$, we should attempt to parse by $S \to CD$. To know which production to pick for a nonterminal, you first figure out all the possible symbols that could appear at the very front of any string that nonterminal can generate.

GOAL:
> We want each parsing function for a nonterminal $X$ to succeed exactly when the next part of the input to pass to $X'$ can be derived from $X$ and to fail otherwise - without backtracking.


More generally consider productions for a nonterminal $X$ $$ X \to a \ | \ \beta$$
and the corresponding parser:
```haskell
parseX (t : ts) = 
	| t ?? -> parseAlpha
	| t ?? -> parseBeta
	| otherwise -> Nothing
```
The question is what should the conditions be on the lookahead symbol t?

The idea of predictive parsing is this:
- Compute the set of terminal symbols that can start strings derived from each alternative, the first set
- If there is a choice between two or more alternatives, insist that the first sets are disjoint(a grammar restriction).
- The right choice can now be made simply by determining to which alternatives first set the input symbol belongs.

```haskell
parseX (t : ts) = 
	| t ∈ firstSet(alpha) -> parseAlpha
	| t ∈ firstSet(beta) -> parseBeta
	| otherwise -> Nothing
```

However, situation could be more involved, possible to derive empty word from nonterminal, empty word of course doesn't begin with any symbol at all.
Assume $\beta \xRightarrow{*} \epsilon$ 
This introduces ambiguity if the lookahead token `t` is not in `first(β)`—because $\beta$ might be skipped entirely. Clearly, in this case the next input symbol could be a terminal that can follow a string derivable from $X$.

We must refine the parser further.
```haskell
parseX (t : ts) = 
	| t ∈ firstSet(alpha) -> parseAlpha
	| t ∈ firstSet(beta) ∪ follow(X)  -> parseBeta
	| otherwise -> Nothing
```

#### First and follow sets
Develop these ideas further. For a **CFG** $G$ = $(N, T, P, S)$:$$

\begin{array}{rll}

first(\alpha) & = & \{ a \in T\mid\alpha \Rightarrow_{G}^*a\beta \} \\

follow(A) & = & \{ a \in T \mid S \Rightarrow_{G}^*\alpha Aa\beta \}

 \cup \{ $ \ |S \Rightarrow_{G}^*\alpha A \}

\end{array}

$$
where $\alpha,\ \beta \in(N\cup T)^*,\ A \in N$, and where $\$$ is a special "***end of input***" marker.

First set: First set of $a$ is the set of **terminals** where $a$ can derive a string that beings with a.
Follow set: Follow set of some nonterminal $A$ consists of all terminals $a$ that can appear immediately after A in some derivation from the start symbol. The End of input marker $\$$ $\in$ $follow(A)$ if there exists a derivation where $A$ appears at the end of a string derived from $S$.

(reason for alpha and beta in follow set shows that there may be more stuff prior to the A and a.)

To illustrate:
$$

\begin{array}{rll}

S & \to & ABC \\

A & \to & aA  & \mid & \epsilon \\

B & \to & b & \mid & \epsilon \\

C & \to & c & \mid & d

\end{array}

$$

The **first** sets are:

$$

\begin{array}{rll}

first(C) & = & \{ c,d \} \\

first(B) & = & \{ b \} \\

first(A) & = & \{ a \}  \\

first(S) & = & first(ABC) \\

 & = & \left[ \text{because }A \Rightarrow^* \epsilon \text{ and }B \Rightarrow^* \epsilon \right]  \\

 &  & first(A) \cup first(B) \cup first(C) \\

 & = & \{ a,b,c,d \}

\end{array}

$$

The **follow** sets are:

$$

\begin{array}{rll}

follow(C) & = & \{ $ \} \\

follow(B) & = & first(C)=\{ c,d \} \\

follow(A) & = & \left[ \text{because }B \Rightarrow^* \epsilon \right] \\

&& first(B) \cup first(C) \\

&=& \{ b,c,d \}

\end{array}

$$
must both be sets of terminals.

#### LL(1) Grammars
Now consider $all$ productions for a nonterminal $A$ in some grammar: $$A \ \to a_n \ | \ a_2 \ | \ ... \  | \ a_n$$
In the parsing function for $A$, on input symbol $t$ we should parse according to $a_i$ if:
- $t \in first(a_i)$ or
- $t \in follow(A), \ if \ a_i \Rightarrow^* \epsilon$ 
Conditions:
- $first(\alpha_{i})\cap first(\alpha_{j})=\emptyset$ for $i \neq j$ , and
- if $\alpha_{i}\Rightarrow^*\epsilon$ for some $i$, then, for all $j \neq i$,
    - $\alpha_{j}\not\Rightarrow^*\epsilon$ and $follow(A)\cap first(\alpha_{j})=\emptyset$
	    - Why? if $a_i \Rightarrow \epsilon$, the parser may choose $A \to \epsilon$ based on $t \in follow(A)$, but if some other $a_j$ starts with $t$, then have ambiguity.
Any grammar which satisfies these above conditions is said to be an $LL(1)$ grammar, and is the ideal class of grammar for predictive parsing, guaranteed to work without **ambiguity** or **backtracking**.

### Nullable nonterminals
To compute the first and follow sets for a Grammar $G$ = $(N, T, P, S)$, first need to know all nonterminals $A \in N$ such that $A \Rightarrow^* \epsilon$, ie the set $N_\epsilon \Subset N$, of ***nullable*** nonterminals.

Let $syms(\alpha)$, function, denote the *set* of symbols in a string $\alpha$.$$\begin{array}{rll}

syms & \in  & (N\cup T)^* \to \mathcal{P}(N \cup T) \\

syms(\epsilon) & = & \emptyset  \\

syms(X\alpha) & = & \{ X \}\cup syms(\alpha )

\end{array}$$
'Get all the grammar symbols in the body of a production.'
The set $N_{\epsilon}$ is the **smallest** solution to the equation

$$

N_{\epsilon}=\{ A\mid A\to\alpha \in P \wedge \forall X \in syms(\alpha) \to X \in N_{\epsilon } \}

$$
A nonterminal $A$ is nullable if it has a production whose entire right-hand side is made up of only nullable symbols.




